#!/usr/bin/env node

const commander = require('commander')
const { version, description } = require('../package.json')
const runBenchmark = require('../lib/runner')
const checkResults = require('../lib/checker')
const formatToText = require('../lib/formatters/text')
const formatToJSON = require('../lib/formatters/json')
const { Instance: Chalk } = require('chalk')

commander
  .version(version)
  .description(description)
  .usage('[options] <URL>')
  .option('-b, --browser <name>', 'web browser to launch', 'chrome')
  .option('-d, --directory <path>', 'root directory to serve from', '.')
  .option('-p, --port <number>', 'port for the server to listen to', 0, toInteger)
  .option('-H, --no-headless', 'show the browser window during the run')
  .option('-S, --no-sandbox', 'pass --no-sandbox to Puppeteer')
  .option('-M, --no-shm', 'pass --disable-dev-shm-usage to Puppeteer')
  .option('-e, --executable', 'set the path to the browser executable')
  .option('-w, --viewport <size>', 'sets the default viewport', '1024x678')
  .option('-t, --timeout <number>', 'benchmark execution timeout [s]', 60, toInteger)
  .option('-f, --format <type>', 'printed results format', 'text')
  .option('-u, --output <path>', 'save all five result artefacts (--save-*)')
  .option('-e, --save-text <file>', 'save results as text')
  .option('-j, --save-json <file>', 'save results as JSON')
  .option('-i, --save-image <file>', 'save PNG screenshot of the page')
  .option('-m, --save-html <file>', 'save HTML markup of the page')
  .option('-l, --save-log <file>', 'save a dump of the console')
  .option('-o, --performance <path>', 'save performance profiles to a directory')
  .option('-r, --error-snapshot <path>', 'save LOG, HTML and PNG snapshots on failure')
  .option('-C, --no-color', 'suppress color output')
  .option('-L, --no-console', 'suppress browser console logging')
  .option('-N, --no-network', 'suppress network request logging')
  .option('-P, --no-progress', 'suppress detailed progress logging')
  .option('-q, --quiet', 'do not print the test results')
  .option('-v, --verbose', 'print progress of the tests')
  .option('--ignore-https-errors', 'force page loading despite of HTTPS errors')
  .option('--no-aborted', 'no benchmark allowed to abort')
  .option('--min-hz', 'minimum allowed frequency [ops/sec]')
  .option('--max-deviation', 'maximum allowed standard deviation')
  .option('--max-mean', 'maximum allowed arithmetic mean [s]')
  .option('--max-moe', 'maximum allowed margin of error')
  .option('--max-rme', 'maximum allowed relative margin of error [%]')
  .option('--max-sem', 'standard error of the mean')
  .option('--max-variance', 'maximum allowed variance')
  .on('--help', () => {
    console.log(`
 Available browsers are "chrome" and "firefox".
 Available formats are "text" and "json".

Examples:

 $ astrobench -vLN --no-aborted test/index.html
 $ astrobench -CS -f json http://localhost:8080/test.html`)
  })
  .parse(process.argv)

function toInteger (value) {
  return +value
}

function validate (condition, message) {
  if (!condition) {
    console.error(message)
    process.exit(1)
  }
}

const url = commander.args[0]
if (!url) {
  commander.help()
}

let {
  browser,
  directory,
  port,
  headless,
  sandbox,
  shm,
  ignoreHttpsErrors: ignoreHTTPSErrors,
  executable,
  viewport,
  timeout,
  format,
  color,
  quiet,
  verbose,
  network,
  console: browserConsole,
  progress,
  output,
  saveText,
  saveJson,
  saveImage,
  saveHtml,
  saveLog,
  performance,
  errorSnapshot,
  aborted,
  minHz,
  maxDeviation,
  maxMean,
  maxMoe,
  maxRme,
  maxSem,
  maxVariance
} = commander
validate(['chrome', 'firefox'].includes(browser),
  'Browser has to be either "chrome" or "firefox".')
validate(port >= 0 && port <= 65535,
  'Port has to be a number greater or equal to zero.')
validate(timeout > 0 && timeout < 86400,
  'Benchmark timeout has to be a number of seconds greater than zero.')
validate(['text', 'json'].includes(format),
  'Format has to be either "text" or "json".')
validate(/^\d+x\d+$/.test(viewport),
  'Viewport has to be formatted like "<width>x<height>".')
const parsedViewport = /^(\d+)x(\d+)$/.exec(viewport)
viewport = { width: +parsedViewport[1], height: +parsedViewport[2] }
if (verbose) {
  verbose = { network, browserConsole, progress }
}

function printResults (suites) {
  if (verbose) {
    console.log()
  }
  const output = format === 'json'
    ? formatToJSON(suites) : formatToText(suites, { color, verbose })
  console.log(output)
}

function printFailedCheck ({ suite, benchmark, message }) {
  const chalk = new Chalk(color !== false ? undefined : { level: 0 })
  console.error(chalk.magenta('Checking results failed:'))
  console.warn(chalk.red(`  ${message}`))
  console.warn(chalk.yellow(`    suite: ${suite}`))
  console.warn(chalk.cyan(`    benchmark: ${benchmark}`))
}

function handleResults (suites) {
  if (!quiet) {
    printResults(suites)
  }
  try {
    if (verbose && verbose.progress !== false) {
      console.log('--- Checking benchmark results')
    }
    checkResults(suites, {
      aborted,
      hz: minHz,
      deviation: maxDeviation,
      mean: maxMean,
      moe: maxMoe,
      rme: maxRme,
      sem: maxSem,
      variance: maxVariance
    })
  } catch (error) {
    if (!quiet) {
      console.log()
    }
    printFailedCheck(error)
    process.exitCode = 2
  }
}

function printError (message) {
  const chalk = new Chalk(color !== false ? undefined : { level: 0 })
  console.error(chalk.magenta('Running benchmarks failed:'))
  console.error(chalk.red(`  ${message}`))
}

function handleError ({ message, stack }) {
  printError(verbose ? stack : message)
  process.exitCode = 1
}

runBenchmark({
  url,
  browser,
  directory,
  port,
  headless,
  sandbox,
  shm,
  ignoreHTTPSErrors,
  viewport,
  executable,
  timeout,
  color,
  verbose,
  output,
  saveText,
  saveJson,
  saveImage,
  saveHtml,
  saveLog,
  performance,
  errorSnapshot
})
  .then(handleResults)
  .catch(handleError)
